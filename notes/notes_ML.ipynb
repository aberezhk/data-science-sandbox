{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics with python intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large, multi-dimensional arrays and matrices, large collection of high-level mathematical functions\n",
    "import numpy as np\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# simple stats models\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# simple charts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pretty charts\n",
    "import seaborn as sns\n",
    "sns.set() # to use as default over matplotlib\n",
    " \n",
    "# sklearn ML library\n",
    "from sklearn.linear_model import LinearRegression # regression model\n",
    "from sklearn.cluster import KMeans # clustering model\n",
    "from sklearn.preprocessing import StandardScaler # standardize the inputs\n",
    "\n",
    "# 3d graphs\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the columns to scale, based on the columns to omit\n",
    "# use list comprehension to iterate over the list\n",
    "columns_to_scale = [x for x in unscaled_inputs.columns.values if x not in columns_to_omit]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scalars: tensor rank 0\n",
    "s = 5\n",
    "\n",
    "# vector: tensor rank 1\n",
    "v = np.array([5,-2,4]) # by default row vector\n",
    "v.reshape(1,3) # reshape row to column vector\n",
    "\n",
    "#matrix: tensor rank 2\n",
    "m = np.array([[3,8,-2], [0,12,1]])\n",
    "m.shape() # shape of the matrix\n",
    "\n",
    "# transpose matrix\n",
    "m.T\n",
    "\n",
    "# multipy vectors \n",
    "v1 = np.array([5,-2,4])\n",
    "v2 = np.array([1,2,3])\n",
    "np.dot(v1,v2) # dot product\n",
    "\n",
    "# tensor rank 3\n",
    "m1 = np.array([[3,8,-2], [0,12,1]])\n",
    "m2 = np.array([[1,2,3], [1,1,1]])\n",
    "t = np.array([m1, m2])\n",
    "\n",
    "# random value\n",
    "observations = 1000\n",
    "xs = np.random.uniform(low=-10,high=10, size=(observations,1)) \n",
    "# 1000 values between -10 and 10, size => matrix shape (observations*1)\n",
    "\n",
    "# combine two vectors into a matrix\n",
    "inputs_array = np.column_stack((xs,zs))\n",
    "\n",
    "# Load the data (csv)\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')\n",
    "\n",
    "\n",
    "# remove headers and separate targets from csv\n",
    "# The inputs are all columns in the csv, except for the first one [:,0]\n",
    "# and the last one [:,-1] (which is our targets)\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "# The targets are in the last column. That's how datasets are conventionally organized.\n",
    "targets_all = raw_csv_data[:,-1]\n",
    "\n",
    "\n",
    "#shuffle the data\n",
    "shuffled_indices = np.arange(unscaled_inputs_all.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "\n",
    "#balance data with categorical targets (1/0)\n",
    "num_one_targets = int(np.sum(targets_all)) # Count how many targets are 1\n",
    "zero_targets_counter = 0 # Set a counter for targets that are 0 \n",
    "indices_to_remove = [] # to store indexes to be removed\n",
    "# Count the number of targets that are 0. \n",
    "# Once there are as many 0s as 1s, mark entries where the target is 0.\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "# delete all indices that we marked \"to remove\" in the loop above.\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0) # from inputs\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0) # from targets\n",
    "\n",
    "# separate data : training, validation, test\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "# Create variables that record the inputs and targets for training\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "# Create variables that record the inputs and targets for validation.\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "# Create variables that record the inputs and targets for test.\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "# create npz file with tensors\n",
    "np.savez('FileName', arr1=inputs_array, arr2=targets_array)\n",
    "# load data from npz file\n",
    "data = np.load('fileName.npz')\n",
    "# convert data from npz tensor to specific format\n",
    "train_inputs = npz['inputs'].astype(np.float) #float\n",
    "\n",
    "# select data based on calculation\n",
    "new_data = np.where(data['ColumnName']> 5, 1, 0) \n",
    "# in new data 1 will be filled for all items, where corresponding item in data was greater than 5, otherwiese 0\n",
    "\n",
    "# convert value to log and add to data frame (pandas)\n",
    "log_x = np.log(data['X'])\n",
    "data['log_x'] = log_x\n",
    "\n",
    "# sum all elements of array\n",
    "np.sum(a)\n",
    "\n",
    "# get exp (covert back from log)\n",
    "x = np.exp(log_x)\n",
    "\n",
    "# absolute value\n",
    "a = np.absolute(b-c)\n",
    "\n",
    "# round float 2 digits\n",
    "np.set_printoptions(formatter={'float': lambda x: \"0:0.2f\".format(x)})\n",
    "\n",
    "# Create an array (so it is easier to calculate the accuracy)\n",
    "cm = np.array(results_df)\n",
    "# Calculate the accuracy of the model\n",
    "accuracy_train = (cm[0,0]+cm[1,1])/cm.sum()\n",
    "accuracy_train\n",
    "\n",
    "\n",
    "# confusion matrix\n",
    "def confusion_matrix(data,actual_values,model):\n",
    "        \n",
    "        # Confusion matrix \n",
    "        \n",
    "        # Parameters\n",
    "        # ----------\n",
    "        # data: data frame or array\n",
    "            # data is a data frame formatted in the same way as your input data (without the actual values)\n",
    "            # e.g. const, var1, var2, etc. Order is very important!\n",
    "        # actual_values: data frame or array\n",
    "            # These are the actual values from the test_data\n",
    "            # In the case of a logistic regression, it should be a single column with 0s and 1s\n",
    "            \n",
    "        # model: a LogitResults object\n",
    "            # this is the variable where you have the fitted model \n",
    "            # e.g. results_log in this course\n",
    "        # ----------\n",
    "        \n",
    "        #Predict the values using the Logit model\n",
    "        pred_values = model.predict(data)\n",
    "        # Specify the bins \n",
    "        bins=np.array([0,0.5,1])\n",
    "        # Create a histogram, where if values are between 0 and 0.5 tell will be considered 0\n",
    "        # if they are between 0.5 and 1, they will be considered 1\n",
    "        cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n",
    "        # Calculate the accuracy\n",
    "        accuracy = (cm[0,0]+cm[1,1])/cm.sum()\n",
    "        # Return the confusion matrix and the accuracy\n",
    "        return cm, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv to data frame\n",
    "data = pd.read_csv('...')\n",
    "\n",
    "# display top 5 rows\n",
    "data.head()\n",
    "\n",
    "# get descriptive statistics \n",
    "data.describe(include='all')\n",
    "\n",
    "# drop a column\n",
    "data = data.drop('ColumnName', axis=1) #axis=1 => column, axis=0 => row\n",
    "\n",
    "# drop rows with missing values\n",
    "data_no_mv = data.dropna(axis=0)\n",
    "\n",
    "# check for missing values\n",
    "data.isnull().sum()\n",
    "\n",
    "#max value in the column\n",
    "data['ColumnName'].max() # use .min() for min value\n",
    "\n",
    "# convert to date from string\n",
    "pd.to_datetime(data['ColumnName'], format='%d/%m/%Y') # format refers to the current format to read from (ex: 28/08/1991)\n",
    "\n",
    "# extract specific part of date\n",
    "data['DateColumn'][itemIndex].month\n",
    "data['DateColumn'][itemIndex].weekday() # 0 Monday, 2 Tuesday ..\n",
    "\n",
    "# apply some function on each item in a column\n",
    "data['ColumnName'].apply(functionName)\n",
    "\n",
    "# get all unique values from column\n",
    "pd.unique(data['ColumnName']) #option 1\n",
    "data['ColumnName'].unique() #option 2\n",
    "# sorted unique values list\n",
    "sorted(data['ColumnName'].unique())\n",
    "\n",
    "# count occurance of a value\n",
    "data['ColumnName'].value_counts()\n",
    "\n",
    "# length of list\n",
    "len(data['ColumnName'])\n",
    "\n",
    "# calculate defined quiantile value and remove data less/more than the value\n",
    "q = data['ColumnName'].quantile(0.99)\n",
    "data = data[data['Price']<q]\n",
    "\n",
    "# reset indexes after removing values from data set\n",
    "data = data.reset_index(drop='True')\n",
    "\n",
    "# for n categories create n-1 dummies\n",
    "data_with_dummies = pd.get_dummies(data['ColumnName'], drop_first=True)\n",
    "\n",
    "# dummies (yes/no example)\n",
    "new_data['ColumnName'] = new_data['ColumnName'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# rename columns\n",
    "df.columns = ['NewName1', 'NewName2']\n",
    "df.columns.values\n",
    "\n",
    "#reorder columns\n",
    "column_names_reordered = ['NewName2', 'NewName1']\n",
    "data = data[column_names_reordered]\n",
    "\n",
    "# join data frames\n",
    "df_1 = pd.DataFrame({'ColumnName1': value}) # put data in data frame\n",
    "df_2 = pd.DataFrame({'ColumnName2': value}) # put data in data frame\n",
    "joined = df_1.join(df_2)\n",
    "\n",
    "# concatinate multiple data frames\n",
    "new_df = pd.concat([df1, df2, df3], axis=1)\n",
    "\n",
    "# To see all rows, we use the relevant pandas syntax\n",
    "pd.options.display.max_rows = 999\n",
    "# Moreover, to make the dataset clear, we can display the result with only 2 digits after the dot \n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "# Finally, we sort by difference in % and manually check the model\n",
    "df_pf.sort_values(by=['Difference%'], ascending = False)\n",
    "\n",
    "# slice the data frame (indexes)\n",
    "x = data.iloc[:,1:4] # select all rows and clumns with index 1,2,3\n",
    "\n",
    "# slice the data frame (column names)\n",
    "x = data.loc[:,'ColumnFirst':'ColumnLast'] # select all rows and clumns from ColumnFirst to ColumnLast including\n",
    "\n",
    "# save to csv\n",
    "df_preprocessed.to_csv('Absenteeism-data-preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple linear regression\n",
    "x = sm.add_constant(x1)\n",
    "results = sm.OLS(y,x).fit()\n",
    "results.summary()\n",
    "\n",
    "# logistic regression\n",
    "x = sm.add_constant(x1)\n",
    "results = sm.Logit(y,x).fit()\n",
    "results.summary()\n",
    "\n",
    "# display table of predicted and actual results\n",
    "results.pred_table()\n",
    "# Some neat formatting to read the table (with pandas) Confusion matrix\n",
    "cm_df = pd.DataFrame(results.pred_table())\n",
    "cm_df.columns = ['Predicted 0','Predicted 1']\n",
    "cm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\n",
    "cm_df\n",
    "\n",
    "# check for multicolliearity\n",
    "# use variance inflation factor VIF\n",
    "# how much larger square root of standard error of the estimate is\n",
    "# comapared to the situation when variable is uncorelated with other predictors\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "variables = data[['X1','X2','X3']]\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF\"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n",
    "vif[\"Features\"] = variables.columns\n",
    "vif # vif==1 => perfect,  between 1 and 5 => ok, >10 => bad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linearity\n",
    "# create multiple plots\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize =(15,3)) #sharey -> share target as y\n",
    "# ax1\n",
    "ax1.scatter(data['X'],data_cleaned['Y'])\n",
    "ax1.set_title('Y per X')\n",
    "# ax2\n",
    "ax1.scatter(data['X'],data_cleaned['Y'])\n",
    "ax1.set_title('Y per X')\n",
    "# axN...\n",
    "plt.show() \n",
    "\n",
    "# scatter plot\n",
    "plt.scatter(y_train, y_hat, alpha=0.2) # alpha - proportional capacity of a point\n",
    "plt.xlabel('y train')\n",
    "plt.ylabel('y hat')\n",
    "fig = plt.plot(y_train, y_train, lw=2, c='orange', label='regression line') # perfect line\n",
    "plt.show()\n",
    "\n",
    "# gradient plot\n",
    "plt.scatter(sat, gpa, c=new_data['FeatureToColorBasedOn'], cmap='RdYlGn')\n",
    "\n",
    "# logistic regression plot\n",
    "reg_log = sm.Logit(y,x)\n",
    "results_log = reg_log.fit()\n",
    "\n",
    "def f(x,b0,b1):\n",
    "    return np.array(np.exp(b0+x*b1) / (1 + np.exp(b0+x*b1)))\n",
    "\n",
    "f_sorted = np.sort(f(x1,results_log.params[0],results_log.params[1]))\n",
    "x_sorted = np.sort(np.array(x1))\n",
    "\n",
    "plt.scatter(x1,y,color='C0')\n",
    "plt.xlabel('SAT', fontsize = 20)\n",
    "plt.ylabel('Admitted', fontsize = 20)\n",
    "plt.plot(x_sorted,f_sorted,color='C8')\n",
    "plt.show()\n",
    "\n",
    "# A method allowing us to create the 3D plot\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(xs, zs, targets)\n",
    "# You can fiddle with the azim parameter to plot the data from different angles\n",
    "ax.view_init(azim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution for some feature\n",
    "sns.distplot(data['ColumnName'])\n",
    "\n",
    "# dendogram\n",
    "sns.clustermap(data, cmap='mako')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the inputs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(inputs)\n",
    "scaled_inputs = scaler.transform(inputs) # usually not recommended to standardize dummy values\n",
    "\n",
    "# SLR : reshape vector data into matrix, only in case of single liner regression (only 1 feature)\n",
    "x_matrix = x.values.reshape(-1,1) # -1 -> 84 objects\n",
    "reg = LinearRegression()\n",
    "reg.fit(x_matrix,y)\n",
    "\n",
    "# split data in training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_inputs, targets, test_size=0.2, random_state=365)\n",
    "\n",
    "# create liner regression\n",
    "reg = LinearRegression()\n",
    "reg.fit(x_train, y_train)\n",
    "\n",
    "# predict values for test set\n",
    "y_hat_test = reg.predict(x_test)\n",
    "\n",
    "# coeficients and intercept\n",
    "reg.intercept_\n",
    "reg.coef_\n",
    "\n",
    "# probabilities\n",
    "predicted_proba = reg.predict_proba(x_test) \n",
    "\n",
    "# calcualate r squared\n",
    "reg.score(x_train, y_train)\n",
    "\n",
    "# extract model weights (with pandas)\n",
    "reg_summary = pd.DataFrame(inputs.columns.values, columns = ['Features'])\n",
    "reg_summary['Weights'] = reg.coef_\n",
    "reg_summary # drop dummies are not there\n",
    "\n",
    "# clustering\n",
    "kmeans = KMeans(2) # cluster with 2 clusters\n",
    "kmeans.fit(x)\n",
    "identified_clusters = kmeans.fit_predict(x)\n",
    "identified_clusters\n",
    "# WCSS -> within cluster sum of squares\n",
    "kmeans.inertia_\n",
    "# find elbow\n",
    "wcss = []\n",
    "for i in range data.shape[0]\n",
    "    kmeans = KMeans(i)\n",
    "    kmeans.fit(data)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "    \n",
    "# import the libraries needed to create the Custom Scaler\n",
    "# note that all of them are a part of the sklearn package\n",
    "# moreover, one of them is actually the StandardScaler module, \n",
    "# so you can imagine that the Custom Scaler is build on it\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the Custom Scaler class\n",
    "class CustomScaler(BaseEstimator,TransformerMixin): \n",
    "    \n",
    "    # init or what information we need to declare a CustomScaler object\n",
    "    # and what is calculated/declared as we do\n",
    "    \n",
    "    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n",
    "        \n",
    "        # scaler is nothing but a Standard Scaler object\n",
    "        self.scaler = StandardScaler(copy,with_mean,with_std)\n",
    "        # with some columns 'twist'\n",
    "        self.columns = columns\n",
    "        self.mean_ = None\n",
    "        self.var_ = None\n",
    "        \n",
    "    \n",
    "    # the fit method, which, again based on StandardScale\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[self.columns], y)\n",
    "        self.mean_ = np.mean(X[self.columns])\n",
    "        self.var_ = np.var(X[self.columns])\n",
    "        return self\n",
    "    \n",
    "    # the transform method which does the actual scaling\n",
    "\n",
    "    def transform(self, X, y=None, copy=None):\n",
    "        \n",
    "        # record the initial order of the columns\n",
    "        init_col_order = X.columns\n",
    "        \n",
    "        # scale all features that you chose when creating the instance of the class\n",
    "        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n",
    "        \n",
    "        # declare a variable containing all information that was not scaled\n",
    "        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n",
    "        \n",
    "        # return a data frame which contains all scaled features and all 'not scaled' features\n",
    "        # use the original order (that you recorded in the beginning)\n",
    "        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data (https://www.tensorflow.org/datasets/catalog/overview)\n",
    "mnist_dataset = tfds.load(name='mnist') # data by default loaded to C:\\Users\\username\\tensorflow_datasets\n",
    "mnist_dataset = tfds.load(name='mnist', as_supervised=True)  # load in tuple structure [input, target]\n",
    "# to extract data\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True) # also load info about dataset\n",
    "# get number of items from info\n",
    "mnist_info.splits['train'].num_examples\n",
    "\n",
    "# custom data scaler (for mnist data set, each pixel originally 0-255 -> convert to 0-1) (or use sklearn.preprocessing)\n",
    "def scale_f(image, label):\n",
    "    image = tf.cast(image, tf.float32) \n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale_f)\n",
    "\n",
    "# cast data type\n",
    "tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "# shuffle the data to be sure order is random\n",
    "BUFFER_SIZE = 10000\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# Linear combinantion + Output = Layer\n",
    "# specify how the model will be laied down\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "])\n",
    "\n",
    "# calculate dot product of the inputs and weights and add biases => np.dot(inputs,weights)+bias\n",
    "tf.keras.layers.Dense(output_size) # default init weights and bias\n",
    "tf.keras.layers.Dense(output_size,\n",
    "                      kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "                      bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n",
    "                      hidden_layer_size=123,\n",
    "                      activation='relu'\n",
    "                     )\n",
    "\n",
    "# flatten matrix to vector\n",
    "tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "\n",
    "# configure model for training\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error') # stochastic gradient descent; L2 norm scaled by number of observations\n",
    "# with custom optimizer\n",
    "custom_optimizer = tf.keras.optimizers.SDG(learning_rate=0.02)\n",
    "model.compile(optimizer=custom_optimizer, loss='mean_squared_error')\n",
    "\n",
    "\n",
    "#fit data to the model and set number of iterations\n",
    "model.fit(inputs, targets, epochs=100, verbose=0)\n",
    "\n",
    "# fit model with early stopping, batch size, \n",
    "model.fit(train_inputs, # train inputs\n",
    "          train_targets, # train targets\n",
    "          batch_size=batch_size, # batch size\n",
    "          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n",
    "          # callbacks are functions called by a task when a task is completed\n",
    "          # task here is to check if val_loss is increasing\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)], # early stopping, let validation loss increase 2 times\n",
    "          validation_data=(validation_inputs, validation_targets), # validation data\n",
    "          verbose = 2 # making sure we get enough information about the training process\n",
    "          )  \n",
    "\n",
    "# get weights\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "bias = model.layers[0].get_weights()[1]\n",
    "\n",
    "# make predictions\n",
    "model.predict_on_batch(data['inputs'])\n",
    "\n",
    "#evaluate model on test data\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pickle the model file\n",
    "with open('model', 'wb') as file:\n",
    "    pickle.dump(reg, file)\n",
    "# pickle the scaler file\n",
    "with open('scaler','wb') as file:\n",
    "    pickle.dump(absenteeism_scaler, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
